{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b975c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import init_notebook_mode, iplot, download_plotlyjs\n",
    "init_notebook_mode(connected = True)\n",
    "sns.set(style='white')\n",
    "sns.set(style='whitegrid', color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d02373f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                   lor\n",
       "0   It is with pleasure that I am writing this let...\n",
       "1   To undertake this task successfully, she must ...\n",
       "2   Copy of www.scholarstrategy.com - Do not redis...\n",
       "3   She is a driven, organized teacher who\\ndevelo...\n",
       "4   Stacy is a perceptive, sharp, quick individual...\n",
       "5   It gives me immense pleasure to recommend Mr M...\n",
       "6   I am extremely happy to recommend Ms. Student ...\n",
       "7   I am pleased to recommend to you Mr. STUDENT N...\n",
       "8   I am extremely happy to recommend Ms. Student ...\n",
       "9   I am writing this letter of recommendation for...\n",
       "10  I am contented to endorse this letter of recom...\n",
       "11  I take immense pleasure to recommend Student N...\n",
       "12  It is a nice thing to know that my student Mr....\n",
       "13   It is with pleasure that I issue this letter ...\n",
       "14  With immense pleasure, I am writing this refer...\n",
       "15  To The Admission Committee: \\n\\nI am glad to e...\n",
       "16  I'm writing to recommend Mary Thompson for the...\n",
       "17  Reference Letter for Archit Gupta\\nI'm very ha...\n",
       "18  It is a pleasure to write a letter of recommen...\n",
       "19  It is an immense pleasure to recommend Ms. Yog...\n",
       "20  I highly recommend Michelle Johnson as a candi...>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/lor.csv\")\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7581a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def lemmatize_string(s):\n",
    "    nltk.download('wordnet')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    list2 = nltk.word_tokenize(s)\n",
    "    lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2])\n",
    "    return lemmatized_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9e239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1264e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "l=[]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "for i in range(21):\n",
    "    l.append(lemmatize_string(df.iloc[i]['lor'].lower()))\n",
    "X = vectorizer.fit_transform(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6466f595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=5, n_init=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89cb26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89db88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " mary\n",
      " wa\n",
      " administrative\n",
      " company\n",
      " addition\n",
      " position\n",
      " solution\n",
      " streaming\n",
      " technical\n",
      " xyz\n",
      " video\n",
      " team\n",
      " industry\n",
      " excellent\n",
      " new\n",
      " service\n",
      " application\n",
      " skill\n",
      " network\n",
      " approach\n",
      " customer\n",
      " ve\n",
      " asia\n",
      " south\n",
      " technology\n",
      " audio\n",
      " project\n",
      " dedication\n",
      " department\n",
      " problem\n",
      " consistently\n",
      " assistant\n",
      " impressed\n",
      " work\n",
      " international\n",
      " graduate\n",
      " program\n",
      " oriented\n",
      " data\n",
      " constantly\n",
      " example\n",
      " client\n",
      " writing\n",
      " idea\n",
      " make\n",
      " strongly\n",
      " expertise\n",
      " intern\n",
      " enterprise\n",
      " health\n",
      "\n",
      "\n",
      "\n",
      "Cluster 1:\n",
      " student\n",
      " life\n",
      " man\n",
      " subject\n",
      " electrical\n",
      " master\n",
      " peer\n",
      " character\n",
      " university\n",
      " worthy\n",
      " feel\n",
      " ability\n",
      " make\n",
      " taught\n",
      " ha\n",
      " unique\n",
      " stand\n",
      " engineering\n",
      " young\n",
      " mr\n",
      " aid\n",
      " nature\n",
      " known\n",
      " aptitude\n",
      " hard\n",
      " hallmark\n",
      " quarter\n",
      " scholastic\n",
      " close\n",
      " true\n",
      " overall\n",
      " adequate\n",
      " reflection\n",
      " analyze\n",
      " having\n",
      " nice\n",
      " laurel\n",
      " enduring\n",
      " diligent\n",
      " accomplish\n",
      " proud\n",
      " devotion\n",
      " financial\n",
      " pursue\n",
      " inquisitive\n",
      " bring\n",
      " confidently\n",
      " institute\n",
      " deeper\n",
      " chosen\n",
      "\n",
      "\n",
      "\n",
      "Cluster 2:\n",
      " student\n",
      " ms\n",
      " subject\n",
      " extremely\n",
      " skill\n",
      " considering\n",
      " situation\n",
      " field\n",
      " graduate\n",
      " recommend\n",
      " ability\n",
      " study\n",
      " known\n",
      " fundamental\n",
      " aid\n",
      " motivation\n",
      " strongly\n",
      " past\n",
      " reverential\n",
      " national\n",
      " behavior\n",
      " names\n",
      " volunteer\n",
      " admirable\n",
      " elder\n",
      " coordinator\n",
      " scheme\n",
      " abreast\n",
      " candidature\n",
      " belt\n",
      " half\n",
      " indicative\n",
      " ha\n",
      " shown\n",
      " enterprising\n",
      " high\n",
      " financial\n",
      " university\n",
      " industrious\n",
      " remarkable\n",
      " sound\n",
      " committed\n",
      " achievement\n",
      " computational\n",
      " beginning\n",
      " foundation\n",
      " offered\n",
      " pursuing\n",
      " happy\n",
      " endeavor\n",
      "\n",
      "\n",
      "\n",
      "Cluster 3:\n",
      " ha\n",
      " like\n",
      " candidate\n",
      " good\n",
      " academic\n",
      " student\n",
      " attentive\n",
      " class\n",
      " annie\n",
      " course\n",
      " higher\n",
      " presented\n",
      " various\n",
      " given\n",
      " knowledge\n",
      " study\n",
      " research\n",
      " work\n",
      " observation\n",
      " year\n",
      " highly\n",
      " university\n",
      " creative\n",
      " future\n",
      " concept\n",
      " graduate\n",
      " skill\n",
      " recommendation\n",
      " topper\n",
      " language\n",
      " pragmatic\n",
      " assistance\n",
      " represent\n",
      " competency\n",
      " immense\n",
      " html\n",
      " lucid\n",
      " success\n",
      " subject\n",
      " admission\n",
      " participation\n",
      " optimistic\n",
      " english\n",
      " field\n",
      " final\n",
      " financial\n",
      " potential\n",
      " discussion\n",
      " association\n",
      " considered\n",
      "\n",
      "\n",
      "\n",
      "Cluster 4:\n",
      " michelle\n",
      " student\n",
      " wa\n",
      " teacher\n",
      " science\n",
      " ha\n",
      " archit\n",
      " manish\n",
      " computer\n",
      " stacy\n",
      " school\n",
      " janet\n",
      " driven\n",
      " year\n",
      " teaching\n",
      " curriculum\n",
      " ability\n",
      " committee\n",
      " based\n",
      " project\n",
      " member\n",
      " manual\n",
      " ideal\n",
      " skill\n",
      " tripura\n",
      " organized\n",
      " development\n",
      " program\n",
      " assignment\n",
      " drive\n",
      " especially\n",
      " make\n",
      " data\n",
      " summer\n",
      " taught\n",
      " person\n",
      " quite\n",
      " develop\n",
      " place\n",
      " peer\n",
      " leader\n",
      " parent\n",
      " extra\n",
      " seema\n",
      " club\n",
      " talent\n",
      " university\n",
      " age\n",
      " advisor\n",
      " superior\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :50]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42cae00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "import pickle\n",
    "with open('model_picle','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3eec696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\risha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, (1, 1172))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Prediction\")\n",
    "X = vectorizer.transform([lemmatize_string(l[16])])\n",
    "predicted = model.predict(X)\n",
    "print(predicted),X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac6bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a26f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
